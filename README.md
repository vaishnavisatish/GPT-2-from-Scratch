# -GPT-2-from-Scratch-with-Instruction-Tuning
Built a GPT-2 language model entirely from scratch using PyTorch and pretrained weights. All core components—including multi-head self-attention, transformer blocks, and layer normalization—were implemented manually. The model was fine-tuned using instruction-based data (Alpaca dataset) and evaluated using Ollama with a score of 80.
